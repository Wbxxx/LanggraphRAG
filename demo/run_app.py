import os
import re
import sys
import json
import dill
import time
import torch
import jieba
import pickle

import streamlit as st
from datetime import datetime
from typing import List, Dict, Tuple, Optional

from langchain.docstore.document import Document
from langchain.retrievers import EnsembleRetriever
from langchain_community.vectorstores import FAISS
from langchain_community.retrievers import BM25Retriever
from langchain_ollama import ChatOllama, OllamaEmbeddings
from langchain_community.tools.tavily_search import TavilySearchResults
from langchain_huggingface import HuggingFacePipeline, HuggingFaceEmbeddings
from langchain.schema import SystemMessage, HumanMessage, AIMessage, FunctionMessage

from FlagEmbedding import LayerWiseFlagLLMReranker

current_dir = os.path.dirname(__file__)  
project_root = os.path.abspath(os.path.join(current_dir, ".."))
sys.path.append(project_root)

from src import config
from src.dag.workflow import build_workflow

# æ‰‹åŠ¨ä¿®æ­£ torch.classes.__path__ï¼Œä»¥è§£å†³æŸäº› Python/Faiss çš„å…¼å®¹æ€§é—®é¢˜
torch.classes.__path__ = [os.path.join(torch.__path__[0], torch.classes.__file__)]

# ========== (1) Streamlit çš„å…¨å±€èµ„æºåˆå§‹åŒ– ==========

def custom_preprocess_func(text):
    text = re.sub(r"[^\u4e00-\u9fa5a-zA-Z0-9_]+", " ", text)
    tokens = jieba.lcut(text)
    return [tok.strip() for tok in tokens if tok.strip()]

def initialize_system():
    """
    åˆå§‹åŒ–ç³»ç»Ÿç»„ä»¶ï¼Œåªåœ¨é¦–æ¬¡è¿è¡Œæ—¶æ‰§è¡Œ:
    - åŠ è½½ LLM & Embeddings
    - åŠ è½½å‘é‡ç´¢å¼• & æ„é€ æ£€ç´¢å™¨
    - åŠ è½½ Workflow DAG
    """
    print("Initializing system components...")

    # åˆå§‹åŒ–LLM
    local_llm_name = 'qwen2.5:14b'
    llm = ChatOllama(model=local_llm_name, temperature=0.7)
    llm_json_mode = ChatOllama(model=local_llm_name, temperature=0.7, format='json')

    # åˆå§‹åŒ–Embeddings
    embedding_model_name = "bge-m3:latest"
    embedding = OllamaEmbeddings(model=embedding_model_name)

    # åŠ è½½æœ¬åœ°ç¨€ç–ç´¢å¼• & å¯†é›†å‘é‡åº“
    sparse_index_path = "./vectordb/bm25_index.pkl"
    dense_faiss_dir = "./vectordb/faiss_index"

    with open(sparse_index_path, "rb") as f:
        sparse_retriever = pickle.load(f)

    vectorstore = FAISS.load_local(dense_faiss_dir, embedding, allow_dangerous_deserialization=True)
    dense_retriever = vectorstore.as_retriever(search_type="similarity", search_kwargs={"k": 5})

    ensemble_retriever = EnsembleRetriever(
        retrievers=[sparse_retriever, dense_retriever],
        weights=[0.5, 0.5]
    )

    # åˆå§‹åŒ–é‡æ’åºæ¨¡å‹
    reranker_model_name = "/mnt/workspace/pretrain_model/BAAI/bge-reranker-v2-minicpm-layerwise/"
    reranker = LayerWiseFlagLLMReranker(reranker_model_name, use_fp16=True)

    # å¯èƒ½çš„å¤–éƒ¨æœç´¢å·¥å…·ï¼ˆå¦‚ web æœç´¢ï¼‰
    web_search_tool = TavilySearchResults(k=3)

    # ä»é¡¹ç›®ä»£ç ä¸­å¯¼å…¥ build_workflow
    current_dir = os.path.dirname(__file__)
    project_root = os.path.abspath(os.path.join(current_dir, ".."))
    sys.path.append(project_root)

    workflow_graph = build_workflow(llm, llm_json_mode, ensemble_retriever, reranker, web_search_tool)

    # æ¨¡æ‹Ÿä¸€äº›å…¨å±€ config
    workflow_config = {"configurable": {"thread_id": "2"}}

    return workflow_graph, workflow_config

# ========== (2) ç¡®ä¿åœ¨ Session State ä¸­åªåˆå§‹åŒ–ä¸€æ¬¡ ==========

def get_global_graph_and_config():
    """
    å°è£…ä¸€ä¸ªå‡½æ•°æ¥è·å¾— graph å’Œ config.
    è‹¥åœ¨ st.session_state ä¸­å°šæœªåˆå§‹åŒ–ï¼Œåˆ™æ‰§è¡Œ initialize_system()
    """
    if "graph" not in st.session_state or "workflow_config" not in st.session_state:
        st.session_state.graph, st.session_state.workflow_config = initialize_system()

    return st.session_state.graph, st.session_state.workflow_config

# ========== (3) å…¶ä½™å¯¹è¯é€»è¾‘ ==========

Conversation = Dict
Message = Dict[str, str]

class ConversationManager:    
    def __init__(self):
        if "conversations" not in st.session_state:
            st.session_state.conversations: List[Conversation] = []
            
        if "current_conv_id" not in st.session_state:
            st.session_state.current_conv_id: Optional[int] = None
    
    def start_new_conversation(self) -> None:
        new_id = len(st.session_state.conversations)
        new_conv = {
            "id": new_id,
            "start_time": datetime.now().strftime("%Y-%m-%d %H:%M:%S"),
            "messages": [],
            "reasoning": ""
        }
        st.session_state.conversations.append(new_conv)
        st.session_state.current_conv_id = new_id
    
    def get_current_conversation(self) -> Optional[Conversation]:
        if st.session_state.current_conv_id is None:
            return None
        return st.session_state.conversations[st.session_state.current_conv_id]
    
    def add_message(self, role: str, content: str) -> None:
        conv = self.get_current_conversation()
        if conv is not None:
            conv["messages"].append({"role": role, "content": content})
    
    def update_reasoning(self, reasoning: str) -> None:
        conv = self.get_current_conversation()
        if conv is not None:
            conv["reasoning"] += reasoning


def generate_response(messages: List[Dict], max_retries: int) -> Tuple[str, str]:
    """
    å°†ç”¨æˆ·/ç³»ç»Ÿæ¶ˆæ¯ä¼ ç»™ workflow_graphï¼Œ
    è·å–æœ€ç»ˆè¾“å‡º + è¿‡ç¨‹æ—¥å¿—
    """
    # ç¡®ä¿æˆ‘ä»¬çš„ graph & config å·²ç»è¢«åˆå§‹åŒ–
    graph, config = get_global_graph_and_config()

    inputs = {
        "messages": [convert_to_langchain_message(m) for m in messages],
        "max_retries": max_retries,
        "log": "ROUTE QUESTION"
    }
    
    reasoning_log = []
    final_answer = ""
    
    for event in graph.stream(inputs, config, stream_mode="values"):
        if "final_answer" in event and event["final_answer"]:
            final_answer = event["final_answer"]
        
        log_entry = process_event_log(event)
        reasoning_log.append(log_entry)

    return final_answer, "\n".join(reasoning_log)


def convert_to_langchain_message(msg: Message):
    role_mapping = {
        "system":  SystemMessage,
        "user":    HumanMessage,
        "assistant": AIMessage,
        "function": FunctionMessage
    }
    return role_mapping[msg["role"]](content=msg["content"])


def process_event_log(event: Dict) -> str:
    """
    å°† event ä¸­çš„æ—¥å¿—å†…å®¹æ‹¼æˆå­—ç¬¦ä¸²ï¼Œæ–¹ä¾¿å­˜å…¥ reason_log
    """
    log_parts = [f"{event['log']}..."] if 'log' in event else []
    
    for msg in event.get('messages', []):
        if isinstance(msg, HumanMessage):
            log_parts.append(f"ç”¨æˆ·æ¶ˆæ¯: {msg.content}")
        elif isinstance(msg, AIMessage):
            log_parts.append(f"åŠ©æ‰‹æ¶ˆæ¯: {msg.content}")
        elif isinstance(msg, FunctionMessage):
            log_parts.append(f"å‡½æ•°è°ƒç”¨: {msg.content}")
    
    return "\n".join(log_parts)


def render_sidebar(conv_manager: ConversationManager):
    with st.sidebar:
        st.markdown("### ğŸ› ï¸ å‚æ•°è®¾ç½®")
        max_retries = st.slider("æœ€å¤§é‡è¯•æ¬¡æ•°", 1, 10, 3)
        
        st.markdown("---")
        if st.button("ğŸ†• å¼€å¯æ–°å¯¹è¯", use_container_width=True):
            conv_manager.start_new_conversation()
            st.rerun()
        
        render_conversation_history(conv_manager)

    # ä¹Ÿå¯ä»¥å°† max_retries è¿”å›å­˜åˆ° session_state
    st.session_state.max_retries = max_retries


def render_conversation_history(conv_manager: ConversationManager):
    st.markdown("### ğŸ“š å†å²å¯¹è¯")
    if not st.session_state.conversations:
        st.write("æš‚æ— å†å²å¯¹è¯")
        return
    
    for conv in reversed(st.session_state.conversations):
        with st.expander(format_conversation_title(conv), expanded=False):
            render_single_conversation(conv)


def format_conversation_title(conv: Conversation) -> str:
    first_question = next(
        (msg["content"] for msg in conv["messages"] if msg["role"] == "user"),
        "æ–°å¯¹è¯"
    )
    return f"{conv['id']+1}. {conv['start_time']} - {truncate_text(first_question)}"


def truncate_text(text: str, length: int = 20) -> str:
    return text[:length] + "..." if len(text) > length else text


def render_single_conversation(conv: Conversation):
    for msg in conv["messages"]:
        role_icon = "ğŸ‘¤" if msg["role"] == "user" else "ğŸ¤–"
        st.markdown(f"{role_icon} **{msg['role']}**: {msg['content']}")


def render_main_interface(conv_manager: ConversationManager):
    st.title("DAG+RAGæ™ºèƒ½é—®ç­”ç³»ç»Ÿ")
    
    current_conv = conv_manager.get_current_conversation()
    if current_conv is None:
        st.info("å½“å‰æ²¡æœ‰æ´»åŠ¨å¯¹è¯ï¼Œè¯·ç‚¹å‡»ä¾§è¾¹æ æŒ‰é’®å¼€å¯æ–°å¯¹è¯ã€‚")
        return
    
    # è¾“å‡ºå·²æœ‰æ¶ˆæ¯
    for msg in current_conv["messages"]:
        render_message_bubble(msg, current_conv)

    # èŠå¤©è¾“å…¥æ¡†
    if user_input := st.chat_input("è¯·è¾“å…¥æ‚¨çš„é—®é¢˜..."):
        conv_manager.add_message("user", user_input)
        
        # æ‹¼æ¥ç»™ DAG çš„æ¶ˆæ¯: system + history
        system_msg = {"role": "system", "content": "You are Jarvis, created by Baoxin Wang. You are a helpful assistant."}
        full_history = [system_msg] + current_conv["messages"]

        with st.spinner("æ­£åœ¨ç”Ÿæˆå›ç­”..."):
            # ä» session_state è·å–é‡è¯•æ¬¡æ•°
            max_retries = st.session_state.get("max_retries", 3)

            # è°ƒç”¨ DAG
            answer, reasoning = generate_response(full_history, max_retries=max_retries)
        
        conv_manager.add_message("assistant", answer)
        conv_manager.update_reasoning(reasoning)

        # è§¦å‘å‰ç«¯ç•Œé¢æ›´æ–°
        st.rerun()


def render_message_bubble(msg: Message, conversation: Conversation):
    """
    æ ¹æ®è§’è‰² (user/assistant) æ˜¾ç¤ºèŠå¤©æ°”æ³¡
    å¦‚æœæ˜¯ assistant æ¶ˆæ¯ï¼Œè¿˜å¯æ˜¾ç¤ºâ€œæŸ¥çœ‹æ¨ç†è¿‡ç¨‹â€
    """
    if msg["role"] == "user":
        with st.chat_message("user"):
            st.markdown(msg["content"])
    else:
        with st.chat_message("assistant"):
            st.markdown(msg["content"])
            # åœ¨è¿™é‡Œæ˜¾ç¤ºæ‰€æœ‰ reasoning
            if conversation.get("reasoning"):
                with st.expander("æŸ¥çœ‹æ¨ç†è¿‡ç¨‹"):
                    st.text(conversation["reasoning"])


def main():
    # ç¡®ä¿ graph åªåˆå§‹åŒ–ä¸€æ¬¡
    get_global_graph_and_config()

    # åˆå§‹åŒ–æˆ–è·å–å¯¹è¯ç®¡ç†å™¨
    conv_manager = ConversationManager()
    if conv_manager.get_current_conversation() is None:
        conv_manager.start_new_conversation()

    # æ¸²æŸ“ä¾§è¾¹æ  & ä¸»ç•Œé¢
    render_sidebar(conv_manager)
    render_main_interface(conv_manager)


if __name__ == "__main__":
    main()